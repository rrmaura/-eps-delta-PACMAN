{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Power\" calculations with Bayesian ($\\epsilon, \\delta$) - PACMAN\n",
    "\n",
    "#### By Roberto-Rafael Maura-Rivero and Sreevidya Ayyar\n",
    "\n",
    "This notebook aims to provide some intuition on perfmorning power calculations if one was to switch from using an RCT to using the ($\\epsilon, \\delta$) - PACMAN Bandit algorithm for comparing several treatment arms in an experiment. \n",
    "\n",
    "First and foremost, recall the relevant definition of power: \n",
    "\n",
    "\"Suppose our parameter of interest is $\\theta$, the average treatment effect of a particular policy. Now, when performing a hypothesis test on this parameter, with specified null and alternative hypotheses (e.g. $H_0: \\theta = 0.3$ and $H_1: \\theta \\ne 0.3$), power is defined as one minus the probability of making a type 2 error.\"\n",
    "\n",
    "$\\text{Power} = 1- P(\\text{fail to reject} H_0|H_A: \\theta \\ne 0.3) = P(\\text{reject} H_0|H_A: \\theta \\ne 0.3)$\n",
    "\n",
    "The difficulty of dealing with power is that the classical definition specifies conditioning on an infitite-dimensional set of possible alternative values for $\\theta$. When implementing power calculations therefore, it is standard practice to focus on a single alternative value of $\\theta$ at a time, i.e.:\n",
    "\n",
    "$\\text{Power} = 1- P(\\text{fail to reject} H_0|H_A: \\theta = \\gamma) = P(\\text{reject} H_0|H_A: \\theta = \\gamma)$\n",
    "\n",
    "for $\\gamma \\in \\{0.1,.02,0.4,0.5\\}$. How to formulate a set of alternatives is generally up to the empirical researcher. One way could be to list values of $\\theta$ that the researcher would like to rule out. Another could be to rule out minimum detestable effects drawn from another, comparable experiment.\n",
    "\n",
    "In our paper, we propose the use of a Bayesian multi-armed bandit algorithm as a way to pin down treatment focus for an RCT. Our algorithm proposes to roll-out multiple treatments simultaneously over sequential experiments, to identify the set of treatment arms that have the highest average treatment effect (with high probability). The intuition is as follows. Instead of conducting a possibly under-powered RCT with 6 treatment arms, we propose that experimenters conduct a pilot study of sequential experiments to identify the set of best-performing arms. Our multi-armed bandit algorithm can be used to conduct a small number of sequential experiments, each requiring much smaller sample sizes than a typical RCT, to identify say the treatment arm(s) with the highest average treatment effect which should be the focus of a larger-scale RCT.\n",
    "\n",
    "Our algorithm works as follows. In the first round of experimentation, we randomize 8 treatments. Our algorithm assesses the performance of each treatment and does two things. Firstly, it discards the 4 worst-performing treatment. This means in the next round, we will only randomize 4 treatments. Secondly, our algorithm uses the information gathered on remaining 4 best-performing treatments to decide how many people should be allocated to each treatment in the next round of the experiment. This is the \"Bayesian\" nature of our algorithm. From here, the algorithm enters into the second round of experimentation, where 4 treatments are randomized. Two are then discarded, and the third round of experimentation is designed only for the 2 remaining treatments. Based on the performance of these 2 treatments in this third round, the algorithm identified the best treatment arm.\n",
    "\n",
    "\n",
    "If we are using this Bayesian multi-armed bandit algorithm though, what is the relevant notion of power? We believe it is not immediately obvious which object to care about, but here is one interpretation we have. Suppose we have 8 treatment arms; that is, 7 treatments and 1 control. \n",
    "\n",
    "Suppose we want to test a very specific alternative hypothesis on the ATE of multiple treatments, e.g. \n",
    "\n",
    "$H: \\\\ \\theta_0 = 0, \\\\ \\theta_1 = 0.1,\\\\ \\theta_2 = 0.2,\\\\ \\theta_3 = 0.3,\\\\ \\theta_4 = 0.4,\\\\ \\theta_5 = 0.5,\\\\ \\theta_6 = 0.6,\\\\ \\theta_7 = 0.7$ \n",
    "\n",
    "Now, we can calculate the probability of \"type 2 error\" as the probability of returning the control as the best treatment.\n",
    "\n",
    "Bandit Power = $Pr$(($\\epsilon, \\delta$)-PACMAN algorithm proposes the control as best treatment | $H$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\roberto\\anaconda3\\envs\\adaptive_ci\\lib\\site-packages (1.21.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\roberto\\anaconda3\\envs\\adaptive_ci\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\roberto\\anaconda3\\envs\\adaptive_ci\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# install + import necessary libraries\n",
    "\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary functions\n",
    "\n",
    "# sample complexity:\n",
    "def sample_complexity(epsilon_L, delta_L, alpha, beta, n_simulations=1000, n_samples=1000):\n",
    "    \n",
    "    n_treatments = len(alpha)\n",
    "    max_n = 100\n",
    "    left = 1\n",
    "    right = max_n\n",
    "    \n",
    "    while left <= right:\n",
    "        n = (left + right) // 2\n",
    "        simulation_with_success = 0\n",
    "        for _ in range(n_simulations):\n",
    "            \n",
    "            thetas = np.random.beta(alpha, beta, size=n_treatments) \n",
    "            highest_theta = np.max(thetas)\n",
    "            is_epsilon_optimal = np.abs(thetas - highest_theta) <= epsilon_L\n",
    "\n",
    "            sum_Y = np.random.binomial(n, thetas)\n",
    "            updated_alpha = alpha + sum_Y\n",
    "            updated_beta = beta + n - sum_Y\n",
    "\n",
    "            bool_survived_treatments = bool_choice_rule(updated_alpha, updated_beta, epsilon_L, n_samples)\n",
    "\n",
    "            success = is_epsilon_optimal * bool_survived_treatments\n",
    "            if np.sum(success) > 0:\n",
    "                simulation_with_success += 1\n",
    "\n",
    "        percentage_success = simulation_with_success / n_simulations\n",
    "\n",
    "        if percentage_success > 1 - delta_L:\n",
    "            right = n - 1\n",
    "        else:\n",
    "            left = n + 1\n",
    "    return left\n",
    "\n",
    "def bool_choice_rule(alpha, beta, epsilon_L, n_samples):\n",
    "    \n",
    "    n_treatments = len(alpha)\n",
    "    \n",
    "    thetas = np.random.beta(alpha[:, np.newaxis], beta[:, np.newaxis], size=(n_treatments, n_samples))\n",
    "    highest_theta = np.max(thetas, axis=0)\n",
    "    is_epsilon_optimal = np.abs(thetas - highest_theta) <= epsilon_L\n",
    "    pr_epsilon_optimal = np.sum(is_epsilon_optimal, axis=1) / n_samples\n",
    "\n",
    "    sorted_indices = np.argsort(pr_epsilon_optimal)[::-1]\n",
    "    top_half_indices = sorted_indices[:n_treatments // 2]\n",
    "    bool_survived_treatments = np.zeros(n_treatments)\n",
    "    bool_survived_treatments[top_half_indices] = 1\n",
    "\n",
    "    return bool_survived_treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ATE of your treatments are: \n",
      "treatment 1 : 0.05\n",
      "treatment 2 : 0.05\n",
      "treatment 3 : 0.05\n",
      "treatment 4 : 0.1\n",
      "treatment 5 : 0.1\n",
      "treatment 6 : 0.1\n",
      "treatment 7 : 0.15\n"
     ]
    }
   ],
   "source": [
    "# researcher-chosen parameters for the algorithm\n",
    "\n",
    "epsilon = 0.05\n",
    "epsilon_L = epsilon/3\n",
    "delta = 0.05\n",
    "delta_L = delta/3\n",
    "\n",
    "number_treatments = 8\n",
    "\n",
    "# hypothesize the average outcome  of each treatment i.e. E(Y|T_i)\n",
    "# set this manually according to how many treatments you wish to test\n",
    "# note - these could also be \"random\" - i.e. drawn from your prior distribution\n",
    "theta = np.zeros(number_treatments)\n",
    "theta[0] = 0.50 # this is the control group average outcome\n",
    "theta[1] = 0.55\n",
    "theta[2] = 0.55\n",
    "theta[3] = 0.55\n",
    "theta[4] = 0.60\n",
    "theta[5] = 0.60\n",
    "theta[6] = 0.60\n",
    "theta[7] = 0.65\n",
    "print (\"the ATE of your treatments are: \")\n",
    "for i in np.arange(1,number_treatments): # ATE of treatments 1-7 are defined relative to the control group average outcome\n",
    "    print (\"treatment \" + str(i) + \" : \" + str(round(theta[i]-theta[0], 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prior\n",
    "\n",
    "# default: prior a uniform distribution for all treatments, i.e. distribution B(1,1)\n",
    "# same prior for each of the eight treatment arms (7 treatments + 1 control)\n",
    "def agnostic_prior():\n",
    "    alphas = np.ones(number_treatments)\n",
    "    betas = np.ones(number_treatments)\n",
    "    return alphas, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:25<00:16,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:41<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# simulations!\n",
    "def simulation(theta, prior):\n",
    "    n_sim = 30  # amount of people used in this simulation\n",
    "    n_remaining_treatments = number_treatments # begin with full set of treatment arms\n",
    "\n",
    "    counter_type_2_error = 0\n",
    "    history_complexity = np.zeros(n_sim)\n",
    "\n",
    "    for i in trange(n_sim):\n",
    "        complexity_simulation = 0\n",
    "        alphas, betas = prior()\n",
    "        remaining_treatments = np.arange(number_treatments)\n",
    "        for village in range(3):\n",
    "            # data recollection\n",
    "            # number of people allocated to each treatment\n",
    "            n_treatment = sample_complexity(epsilon_L=epsilon_L,\n",
    "                        delta_L=delta_L,\n",
    "                        alpha=alphas,\n",
    "                        beta=betas,\n",
    "                        n_simulations=100)\n",
    "            complexity_simulation += n_treatment\n",
    "            \n",
    "            # sample each treatment n_treatment times from a binomial\n",
    "            # distribution with probability theta_i\n",
    "            # for i in range(8):\n",
    "            #     outcome_treatment_i = np.random.binomial(n_treatment, theta[i])\n",
    "            outcome_treatment = np.random.binomial(n_treatment, theta[remaining_treatments])\n",
    "\n",
    "            # update prior (now this is posterior distribution from where each ATE is drawn)\n",
    "            # for i in range(8):\n",
    "            #     alphas[i] += outcome_treatment_i\n",
    "            #     betas[i] += n_treatment - outcome_treatment_i\n",
    "            alphas[remaining_treatments] = alphas[remaining_treatments] + outcome_treatment\n",
    "            betas[remaining_treatments] = betas[remaining_treatments] + n_treatment - outcome_treatment\n",
    "\n",
    "            # choose which treatments survive\n",
    "            # to do that, we first calculate the probability of being epsilon optimal\n",
    "            \n",
    "            # SREE: i do not see this step? how are we updating remaining_treatments?\n",
    "            # Roberto: My bad, idk what happened there. I added it back in now.\n",
    "\n",
    "            bool_survived_treatments = bool_choice_rule(alphas[remaining_treatments], \n",
    "                                                        betas[remaining_treatments], \n",
    "                                                        epsilon_L, \n",
    "                                                        n_samples=100)\n",
    "            remaining_treatments = remaining_treatments[bool_survived_treatments == 1]\n",
    "\n",
    "            # update remaining treatments\n",
    "            n_remaining_treatments = len(remaining_treatments)\n",
    "            if int(n_remaining_treatments) == 1:\n",
    "                # if there is only one treatment left, it is the best one\n",
    "                # and we can stop the simulation\n",
    "                break\n",
    "\n",
    "        history_complexity[i] = complexity_simulation\n",
    "\n",
    "        if 0 in remaining_treatments: \n",
    "            print(remaining_treatments)\n",
    "            counter_type_2_error += 1\n",
    "    return history_complexity, counter_type_2_error\n",
    "\n",
    "history_complexity, counter_type_2_error = simulation(theta, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon is: 0.05\n",
      "delta is: 0.05\n",
      "the average complexity of the simulations is: 45.5\n",
      "the 10th percentile of the complexity is: 22.400000000000002\n",
      "the 90th percentile of the complexity is: 83.0\n",
      "the probability of type 2 error is: 0.06666666666666667\n",
      "The \"bandit power\" is 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"epsilon is: \" + str(epsilon))\n",
    "print(\"delta is: \" + str(delta))\n",
    "\n",
    "# tell me the average complexity of the simulations and relevant quantiles\n",
    "print(\"the average complexity of the simulations is: \" + str(np.mean(history_complexity)))\n",
    "print(\"the 10th percentile of the complexity is: \" + str(np.percentile(history_complexity, 10)))\n",
    "print(\"the 90th percentile of the complexity is: \" + str(np.percentile(history_complexity, 90)))\n",
    "\n",
    "print (\"the probability of type 2 error is: \" + str(counter_type_2_error / n_sim))\n",
    "print (\"The \\\"bandit power\\\" is \" + str(1 - counter_type_2_error / n_sim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After playing around with this simulation, you might find some particular values where the power of the bandit algorithm is quite bad (a high number), while the epsilon-delta are both still very small values. How is this possible?\n",
    "\n",
    "\n",
    "The key here is that the ($\\epsilon, \\delta$) depend on your prior beliefs about the treatment effects. In this notebook, our default is to assume a uniform prior, but you can change it to whatever you want. \n",
    "\n",
    "The uniform prior is uninformative, and so indicates that within a given support, the researcher has no information on what the treatment effect is likely to be. If however you believe (for example) that your treatment effects will be fairly small, and you set the alternative hypothesis to be: \n",
    "\n",
    "$H: \\\\ \\theta_0 = 0, \\\\ \\theta_1 = 0.1,\\\\ \\theta_2 = 0.2,\\\\ \\theta_3 = 0.3,\\\\ \\theta_4 = 0.4,\\\\ \\theta_5 = 0.5,\\\\ \\theta_6 = 0.6,\\\\ \\theta_7 = 0.7$ \n",
    "\n",
    "then you should reflect that by formulating a prior that puts more density around 0-0.7, and less outside this region. Including such information in your prior will have a lot of effect on the algorithm's performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simulation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6840\\3960201705.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mhistory_complexity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter_type_2_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextreme_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magnostic_prior\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'simulation' is not defined"
     ]
    }
   ],
   "source": [
    "extreme_theta = np.zeros(number_treatments)\n",
    "extreme_theta[0] = 0.5 # this is the control group average outcome\n",
    "extreme_theta[1] = 0.5\n",
    "extreme_theta[2] = 0.5\n",
    "extreme_theta[3] = 0.5\n",
    "extreme_theta[4] = 0.5\n",
    "extreme_theta[5] = 0.5\n",
    "extreme_theta[6] = 0.5\n",
    "extreme_theta[7] = 0.551\n",
    "\n",
    "# technically ATE > epsilon, yet, we are going to have problems with power because of agnostic prior\n",
    "\n",
    "history_complexity, counter_type_2_error = simulation(extreme_theta, agnostic_prior)\n",
    "\n",
    "# TODO: define reasonable prior. Plot agnostic prior and reasonable prior \n",
    "# explain the intuition that alpha/(alpha+beta) is like the probability of success\n",
    "# a reasonable prior would have a high alpha and high beta for all treatments and \n",
    "# same alpha/(alpha + beta) for all treatments\n",
    "\n",
    "def reasonable_prior():\n",
    "    raise NotImplementedError\n",
    "\n",
    "history_complexity, counter_type_2_error = simulation(extreme_theta, reasonable_prior)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
