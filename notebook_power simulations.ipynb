{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Power\" calculations with Bayesian ($\\epsilon, \\delta$) - PACMAN\n",
    "\n",
    "By Roberto-Rafael Maura-Rivero \n",
    "\n",
    "This notebook has the goal of helping applied researchers get a sense of how the power calculations would translate from using an RCT to using the ($\\epsilon, \\delta$) - PACMAN Bandit algorithm. \n",
    "\n",
    "First of all, let's revisit what is the definition of Power: \n",
    "\n",
    "In the simplest escenario, where we care about $\\theta$ being the ATE of a particular policy, and we have a very specific alternative hypothesis in mind (e.g. $H_A: \\theta = 0.3$), power is defined as 1 - probability of type 2 error \n",
    "\n",
    "$\\text{power} = 1-\\beta = P(\\text{reject } H_0|H_A: \\theta = 0.3)$\n",
    "\n",
    "In our paper, we suggest to deviate from the RCT and instead consider multiple treatments simultaneously over multiple experiments. The intuition is that, instead of organizing 3 RCTs to compare 3 different treatments, we sequentially run 3 experiments, with 8 treatments, then 4 and then 2. This way, we can compare 8 treatments in total, instead of 3.\n",
    "\n",
    "(explain bandit algorithm here)\n",
    "\n",
    "But if we are using bandits, what should we consider as \"power\"? It is not immediately obvious which object we should care about, but an intuitive suggestion is the following: \n",
    "\n",
    "You consider 8 bandit arms, that is 7 treatments and 1 control. \n",
    "\n",
    "Consider a very specific hypothesis on the ATE of multiple treatments \n",
    "\n",
    "e.g. \n",
    "\n",
    "$H: \\\\ \\theta_0 = 0, \\\\ \\theta_1 = 0.1,\\\\ \\theta_2 = 0.2,\\\\ \\theta_3 = 0.3,\\\\ \\theta_4 = 0.4,\\\\ \\theta_5 = 0.5,\\\\ \\theta_6 = 0.6,\\\\ \\theta_7 = 0.7$ \n",
    "\n",
    "Now, we can calculate the probability of \"type 2 error\" as the probability of returning the control as the best treatment.\n",
    "\n",
    "\"bandit power\" = $Pr$( algorithm ($\\epsilon, \\delta$)-PACMAN returns control as best treatment | $H$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\roberto\\anaconda3\\envs\\adaptive_ci\\lib\\site-packages (1.21.6)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\roberto\\anaconda3\\envs\\adaptive_ci\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 45\n"
     ]
    }
   ],
   "source": [
    "# Necessary functions\n",
    "def sample_complexity(epsilon_L, delta_L, alpha, beta, n_simulations=1000, n_samples=1000):\n",
    "    n_treatments = len(alpha)\n",
    "    MAX_N = 100\n",
    "\n",
    "    left = 1\n",
    "    right = MAX_N\n",
    "    while left <= right:\n",
    "        n = (left + right) // 2\n",
    "\n",
    "        simulation_with_success = 0\n",
    "        for _ in range(n_simulations):\n",
    "            thetas = np.random.beta(alpha, beta, size=n_treatments)\n",
    "            highest_theta = np.max(thetas)\n",
    "            is_epsilon_optimal = np.abs(thetas - highest_theta) <= epsilon_L\n",
    "\n",
    "            sum_Y = np.random.binomial(n, thetas)\n",
    "            updated_alpha = alpha + sum_Y\n",
    "            updated_beta = beta + n - sum_Y\n",
    "\n",
    "            bool_survived_treatments = bool_choice_rule(updated_alpha, updated_beta, epsilon_L, n_samples)\n",
    "\n",
    "            success = is_epsilon_optimal * bool_survived_treatments\n",
    "            if np.sum(success) > 0:\n",
    "                simulation_with_success += 1\n",
    "\n",
    "        percentage_success = simulation_with_success / n_simulations\n",
    "\n",
    "        if percentage_success > 1 - delta_L:\n",
    "            right = n - 1\n",
    "        else:\n",
    "            left = n + 1\n",
    "\n",
    "    return left\n",
    "\n",
    "def bool_choice_rule(alpha, beta, epsilon_L, n_samples):\n",
    "    n_treatments = len(alpha)\n",
    "    thetas = np.random.beta(alpha[:, np.newaxis], beta[:, np.newaxis], size=(n_treatments, n_samples))\n",
    "    highest_theta = np.max(thetas, axis=0)\n",
    "    is_epsilon_optimal = np.abs(thetas - highest_theta) <= epsilon_L\n",
    "    pr_epsilon_optimal = np.sum(is_epsilon_optimal, axis=1) / n_samples\n",
    "\n",
    "    sorted_indices = np.argsort(pr_epsilon_optimal)[::-1]\n",
    "    top_half_indices = sorted_indices[:n_treatments // 2]\n",
    "    bool_survived_treatments = np.zeros(n_treatments)\n",
    "    bool_survived_treatments[top_half_indices] = 1\n",
    "\n",
    "    return bool_survived_treatments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ATE of your treatments are: \n",
      "treatment 1 : 0.05\n",
      "treatment 2 : 0.05\n",
      "treatment 3 : 0.05\n",
      "treatment 4 : 0.1\n",
      "treatment 5 : 0.1\n",
      "treatment 6 : 0.1\n",
      "treatment 7 : 0.15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epsilon = 0.05\n",
    "epsilon_L = epsilon/3\n",
    "delta = 0.05\n",
    "delta_L = delta/3\n",
    "\n",
    "# HYPOTHESIS of the average outcome  of each treatment i.e. E(Y|T_i)\n",
    "theta = np.zeros(8)\n",
    "theta[0] = 0.5\n",
    "theta[1] = 0.55\n",
    "theta[2] = 0.55\n",
    "theta[3] = 0.55\n",
    "theta[4] = 0.60\n",
    "theta[5] = 0.60\n",
    "theta[6] = 0.60\n",
    "theta[7] = 0.65\n",
    "\n",
    "print (\"the ATE of your treatments are: \")\n",
    "for i in np.arange(1,8):\n",
    "    print (\"treatment \" + str(i) + \" : \" + str(round(theta[i]-theta[0], 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior is a uniform distribution for all treatments,i.e. distribution B(1,1)\n",
    "\n",
    "# parameters of the prior beta distributions\n",
    "alphas = np.zeros(8)\n",
    "betas = np.zeros(8)\n",
    "\n",
    "for i in range(8): \n",
    "    alphas[i] = 1\n",
    "    betas[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [02:27<41:03:49, 147.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\471976376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbetas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                     n_simulations=10000)\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mcomplexity_simulation\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_treatment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\2034809955.py\u001b[0m in \u001b[0;36msample_complexity\u001b[1;34m(epsilon_L, delta_L, alpha, beta, n_simulations, n_samples)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mupdated_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msum_Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mbool_survived_treatments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool_choice_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_epsilon_optimal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbool_survived_treatments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\2034809955.py\u001b[0m in \u001b[0;36mbool_choice_rule\u001b[1;34m(alpha, beta, epsilon_L, n_samples)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbool_choice_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mn_treatments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_treatments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mhighest_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mis_epsilon_optimal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhighest_theta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mepsilon_L\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# SIMULATIONS\n",
    "n_sim = 1000\n",
    "n_remaining_treatments = 8\n",
    "remaining_treatments = np.arange(8)\n",
    "\n",
    "counter_type_2_error = 0\n",
    "history_complexity = np.zeros(n_sim)\n",
    "for i in trange(n_sim):\n",
    "    # amount of people used in this simulation\n",
    "    complexity_simulation = 0\n",
    "    for village in range(3):\n",
    "        # data recolection\n",
    "\n",
    "        # number of people allocated to each treatment\n",
    "        n_treatment = sample_complexity(epsilon_L=epsilon_L,\n",
    "                    delta_L=delta_L,\n",
    "                    alpha=alphas,\n",
    "                    beta=betas,\n",
    "                    n_simulations=10000)\n",
    "        complexity_simulation += n_treatment\n",
    "        \n",
    "        # sample each treatment n_treatment times from a binomial\n",
    "        # distribution with probability theta_i\n",
    "        for i in range(8):\n",
    "            data_treatment_i = np.random.binomial(n_treatment, theta[i])\n",
    "\n",
    "        # update prior \n",
    "        for i in range(8):\n",
    "            alphas[i] += data_treatment_i\n",
    "            betas[i] += n_treatment - data_treatment_i\n",
    "\n",
    "        # choose which treatments survive. \n",
    "        # to do that, we first calculate the probability of being epsilon optimal \n",
    "\n",
    "        # update remaining treatments\n",
    "        n_remaining_treatments = len(remaining_treatments)\n",
    "        if n_remaining_treatments == 1:\n",
    "\n",
    "            # if there is only one treatment left, it is the best one\n",
    "            # and we can stop the simulation\n",
    "            break\n",
    "\n",
    "    history_complexity[i] = complexity_simulation\n",
    "\n",
    "    if 0 in remaining_treatments: \n",
    "        counter_type_2_error += 1\n",
    "\n",
    "\n",
    "print(\"epsilon is: \" + str(epsilon))\n",
    "print(\"delta is: \" + str(delta))\n",
    "\n",
    "# tell me the average complexity of the simulations and relevant quantiles\n",
    "print(\"the average complexity of the simulations is: \" + str(np.mean(history_complexity)))\n",
    "print(\"the 10th percentile of the complexity is: \" + str(np.percentile(history_complexity, 10)))\n",
    "print(\"the 90th percentile of the complexity is: \" + str(np.percentile(history_complexity, 90)))\n",
    "\n",
    "print (\"the probability of type 2 error is: \" + str(counter_type_2_error / n_sim))\n",
    "print (\"The \\\"bandit power\\\" is \" + str(1 - counter_type_2_error / n_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 50/1000 [2:46:11<52:37:42, 199.43s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\3457466210.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m                                         \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                                         \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbetas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                                         n_simulations=10000)\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mcomplexity_simulation\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_treatment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\2034809955.py\u001b[0m in \u001b[0;36msample_complexity\u001b[1;34m(epsilon_L, delta_L, alpha, beta, n_simulations, n_samples)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mupdated_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msum_Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mbool_survived_treatments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool_choice_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdated_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdated_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_epsilon_optimal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbool_survived_treatments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25104\\2034809955.py\u001b[0m in \u001b[0;36mbool_choice_rule\u001b[1;34m(alpha, beta, epsilon_L, n_samples)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbool_choice_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mn_treatments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mthetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_treatments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mhighest_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mis_epsilon_optimal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthetas\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhighest_theta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mepsilon_L\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "# SIMULATIONS\n",
    "n_sim = 1000\n",
    "n_remaining_treatments = 8\n",
    "remaining_treatments = np.arange(8)\n",
    "\n",
    "counter_type_2_error = 0\n",
    "history_complexity = np.zeros(n_sim)\n",
    "\n",
    "for i in trange(n_sim):\n",
    "    # amount of people used in this simulation\n",
    "    complexity_simulation = 0\n",
    "    alphas = np.ones(8)\n",
    "    betas = np.ones(8)\n",
    "    theta = np.random.rand(8)  # Random theta values for demonstration purposes\n",
    "    epsilon_L = 0.01\n",
    "    delta_L = 0.01\n",
    "\n",
    "    for village in range(3):\n",
    "        # data recolection\n",
    "        n_treatment = sample_complexity(epsilon_L=epsilon_L,\n",
    "                                        delta_L=delta_L,\n",
    "                                        alpha=alphas,\n",
    "                                        beta=betas,\n",
    "                                        n_simulations=10000)\n",
    "        complexity_simulation += n_treatment\n",
    "\n",
    "        # sample each treatment n_treatment times from a binomial\n",
    "        data_treatments = np.random.binomial(n_treatment, theta)\n",
    "\n",
    "        # update prior\n",
    "        alphas += data_treatments\n",
    "        betas += n_treatment - data_treatments\n",
    "\n",
    "        # update remaining treatments\n",
    "        n_remaining_treatments = len(remaining_treatments)\n",
    "        if n_remaining_treatments == 1:\n",
    "            # if there is only one treatment left, it is the best one\n",
    "            # and we can stop the simulation\n",
    "            break\n",
    "\n",
    "    history_complexity[i] = complexity_simulation\n",
    "\n",
    "    if 0 in remaining_treatments:\n",
    "        counter_type_2_error += 1\n",
    "\n",
    "print(\"epsilon is: \" + str(epsilon_L))\n",
    "print(\"delta is: \" + str(delta_L))\n",
    "\n",
    "# tell me the average complexity of the simulations and relevant quantiles\n",
    "print(\"the average complexity of the simulations is: \" + str(np.mean(history_complexity)))\n",
    "print(\"the 10th percentile of the complexity is: \" + str(np.percentile(history_complexity, 10)))\n",
    "print(\"the 90th percentile of the complexity is: \" + str(np.percentile(history_complexity, 90)))\n",
    "\n",
    "print(\"the probability of type 2 error is: \" + str(counter_type_2_error / n_sim))\n",
    "print(\"The \\\"bandit power\\\" is \" + str(1 - counter_type_2_error / n_sim))\n",
    "\n",
    "# it takes 3 hours to calculate 50 simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you get a graph with the bandit power of the test for each sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with this you might say: \n",
    "Question:\n",
    "Hey! I tried something like \n",
    "\n",
    "$H: \\\\ \\theta_0 = 0, \\\\ \\theta_1 = 0.1,\\\\ \\theta_2 = 0.2,\\\\ \\theta_3 = 0.3,\\\\ \\theta_4 = 0.4,\\\\ \\theta_5 = 0.5,\\\\ \\theta_6 = 0.6,\\\\ \\theta_7 = 0.7$ \n",
    "\n",
    "Yet the badit power is quite bad and the epsilon delta is quite good. How is that possible?\n",
    "\n",
    "Answer:\n",
    "Well, you see, the ($\\epsilon, \\delta$) depend on your prior believes about the treatment effects. In this notebook, we assumed a uniform prior, but you can change it to whatever you want. \n",
    "\n",
    "And, in fact, if you believe the effect of the treatment is fairly small, then you should reflect that in your prior. \n",
    "\n",
    "In the next cells we will show you how to do that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive_CI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
